# app.py ................
from math import inf
import re
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
from typing import List
import requests
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import io
import logging
import gc
from cidindex import CIDIndex
from pyspark.sql import SparkSession
import os


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

app = FastAPI()

# Initialize global variables for index
# Add HospitalID for testing (because there is no repeated PatientID in the test data)

app.state.index_cids = {
    'PatientID': None,
    'HospitalID': None,
    'Age': None,   
}

# change the default index attribute to test the index
# default_index_attribute = 'PatientID'


logger.info("Starting FastAPI application")

logger.info("Initializing Spark Session")
spark_master = os.environ.get("SPARK_MASTER", "spark://spark-master:7077")
print(f"Spark master: {spark_master}")
spark_driver_host = os.environ.get("SPARK_DRIVER_HOST", "localhost")
print(f"Spark driver host: {spark_driver_host}")

spark = (
    SparkSession.builder.appName("FastAPISparkDriver")
    .master(spark_master)
    .config("spark.blockManager.port", "10025")
    .config("spark.driver.blockManager.port", "10026")
    .config("spark.driver.host", spark_driver_host)
    .config("spark.driver.port", "10027")
    .config("spark.python.worker.reuse", "true")
    .config("spark.pyspark.python", "/usr/bin/python3")
    .config("spark.pyspark.driver.python", "/usr/bin/python3")
    .getOrCreate()
)
logger.info(f"Spark Session created: {spark.sparkContext.appName}")


@app.post("/upload/patient-data")
async def upload_patient_data(file: UploadFile = File(...)):
    """
    Upload patient_data.csv to IPFS in Parquet format
    Returns a Content Identifier (CID) for the uploaded file
    """
    logger.info("POST /upload/patient-data - Processing patient data upload")

    try:
        # Read CSV file content into memory
        logger.info("Reading uploaded CSV file into memory")
        content = await file.read()

        # Process CSV in memory
        logger.info("Converting CSV to DataFrame")
        csv_buffer = io.BytesIO(content)
        df = pd.read_csv(csv_buffer, dtype={"PatientID": str, "HospitalID": str, "Age": int})
        
        # Get values for the default index attribute
        indexed_values = {}
        for index_key in app.state.index_cids.keys():
            if index_key in df.columns:
                indexed_values[index_key] = set(df[index_key].values)
                # logger.info(f"Indexed values for {index_key}: {indexed_values[index_key]}")
            else:
                logger.warning(f"Index attribute {index_key} not found in DataFrame columns")
        
        # Clear initial content and CSV buffer
        del content
        csv_buffer.close()
        del csv_buffer

        # Convert DataFrame to Parquet in memory
        logger.info("Converting DataFrame to Parquet format in memory")
        parquet_buffer = io.BytesIO()
        table = pa.Table.from_pandas(df)

        # Clear DataFrame and table after conversion
        del df

        pq.write_table(table, parquet_buffer)
        del table

        # Reset buffer position to beginning
        parquet_buffer.seek(0)

        # Upload to IPFS
        ipfs_api_url = "http://localhost:5001/api/v0/add"
        logger.info(f"Uploading Parquet data to IPFS node at {ipfs_api_url}")

        response = requests.post(
            ipfs_api_url,
            files={
                "file": (
                    "patient_data.parquet",
                    parquet_buffer,
                    "application/octet-stream",
                )
            },
        )
        response.raise_for_status()
        cid = response.json()["Hash"]
        logger.info(f"Patient data uploaded to IPFS with CID: {cid}")

        # Clear the parquet buffer
        parquet_buffer.close()
        del parquet_buffer
        
        
        for index_key in indexed_values.keys():
            # Create/update index for the default index attribute
            logger.info(f"Creating/updating index for attribute: {index_key}")
            if index_key not in app.state.index_cids or app.state.index_cids[index_key] is None:
                # Create new index
                logger.info(f"Creating new index for {index_key}")
                index = CIDIndex([(val, cid) for val in indexed_values[index_key]])
                # print the index type
                logger.info(f"Index type: {index.index_type}")
            else:
                index = retrieve_index(index_key)
                # Update existing index
                logger.info(f"Updating existing index for {index_key}")
                index.update([(val, cid) for val in indexed_values[index_key]])
            # Serialize and upload index to IPFS
            logger.info(f"Serializing index for {index_key}")
            serialized_index = index.dump()
            # Put index on IPFS
            logger.info(f"Uploading index for {index_key} to IPFS")
            response = requests.post(
                ipfs_api_url, 
                files={"file": (f"{index_key}_index", serialized_index, "application/octet-stream")}
            )
            response.raise_for_status()
            index_cid = response.json()["Hash"]
            logger.info(f"Index for {index_key} uploaded to IPFS with CID: {index_cid}")
            # Update global state with new index CID
            app.state.index_cids[index_key] = index_cid
            logger.info(f"Updated index CID for {index_key} in global state")
            # Clear the serialized index
            serialized_index.close()
            del serialized_index
            # Clear the index object
            del index

        # Explicitly trigger garbage collection
        gc.collect()

        logger.info("Memory buffers cleared")

        # Return the CID
        return {"data_message": "Patient data uploaded successfully at " + cid, "index_message": "Successfully created index for " + str(app.state.index_cids)}
    except Exception as e:
        # Make sure to clean up memory even if an error occurs
        logger.error(f"Error processing patient data: {str(e)}")
        gc.collect()
        return {"error": f"Failed to process and upload data: {str(e)}"}
# Define request model
class QueryRequest(BaseModel):
    default_index_attribute: str = str(app.state.index_cids.keys())  # Default to first index attribute
    index_attribute: str = 'HospitalID'
    query: str = "select * from patient_data where HospitalID = 'HOSP-003'"


@app.post("/query")
async def query_distributed(request: QueryRequest):
    """
    Distributed query across multiple IPFS CIDs with parallel data retrieval
    and centralized query execution
    
    Example query: select * from patient_data where HospitalID = 'HOSP-003'
    """
    logger.info("POST /query - Processing distributed query across CIDs")
    
    index_attribute = request.index_attribute
    index = retrieve_index(index_attribute)
    if not index:
        logger.error(f"Index for {index_attribute} not found")
        return {"error": f"Index for {index_attribute} not found"}
    logger.info(f"Index for {index_attribute} retrieved successfully")
    
    cids = query_index(index, request.query, index_attribute)
    # check the cids
    logger.info(f"Query returned {len(cids)} CIDs")
    logger.info(f"Query CIDs: {cids}")

    try:
        # cids = request.cids
        query = request.query

        if not cids or not query:
            return {"message": "No data to process or query is empty"}
            
        logger.info(f"Processing {len(cids)} CIDs with query: {query}")

        # Create an RDD from the CIDs list
        cids_rdd = spark.sparkContext.parallelize(cids)

        # Process each CID in parallel to fetch and transform data
        def fetch_and_process_data(cid):
            import requests
            import base64
            import socket
            import os
            import io
            import pandas as pd
            import pyarrow.parquet as pq

            # Get worker information
            hostname = socket.gethostname()
            worker_pid = os.getpid()
            worker_id = f"{hostname}-{worker_pid}"

            try:
                # Log at the beginning of processing
                print(f"Worker {worker_id} starting to fetch CID: {cid}")

                # Fetch data from IPFS
                ipfs_api_url = f"http://localhost:5001/api/v0/cat"
                response = requests.post(ipfs_api_url, params={"arg": cid}, timeout=10)

                if response.status_code != 200:
                    print(
                        f"Worker {worker_id} failed to fetch CID: {cid} with status code: {response.status_code}"
                    )
                    return []  # Return empty list for concat

                # Process the Parquet data
                binary_data = response.content
                parquet_buffer = io.BytesIO(binary_data)
                table = pq.read_table(parquet_buffer)
                df = table.to_pandas()

                # Add source CID and worker info
                df["source_cid"] = cid
                df["worker_id"] = worker_id

                print(f"Worker {worker_id} successfully processed CID: {cid}")

                # Return as list of dictionaries (records)
                return df.to_dict(orient="records")

            except Exception as e:
                print(f"Worker {worker_id} encountered error with CID {cid}: {str(e)}")
                return []  # Return empty list for failed CIDs

        # Map each CID to its processed records and flatten the results
        all_records = cids_rdd.flatMap(fetch_and_process_data).collect()

        # If no data was processed, return error
        if not all_records:
            return {"error": "Failed to process any data from IPFS"}

        # Create a Spark DataFrame directly from all records
        spark_df = spark.createDataFrame(all_records)

        # Extract worker assignments for reporting
        worker_data = spark_df.select("source_cid", "worker_id").distinct().collect()
        worker_assignments = {
            row["source_cid"]: row["worker_id"] for row in worker_data
        }
        processed_cids = list(worker_assignments.keys())

        # Register as temporary view
        spark_df.createOrReplaceTempView("patient_data")

        # Execute the SQL query
        logger.info(f"Executing query: {query}")
        result_df = spark.sql(query)

        # Convert results to JSON
        result_json = result_df.toJSON().collect()
        result_json = [eval(record.replace("null", "None")) for record in result_json]

        logger.info(
            f"Query completed successfully, returning {len(result_json)} records"
        )

        return {
            "message": "Distributed query executed successfully",
            "cids_processed": len(processed_cids),
            "worker_assignments": worker_assignments,
            "record_count": len(result_json),
            "results": result_json,
        }

    except Exception as e:
        logger.error(f"Error processing distributed query: {str(e)}")
        return {"error": f"Failed to process distributed query: {str(e)}"}
    
    
def retrieve_index(name):
    logger.info(f"Retrieving index for {name}")
    if name not in app.state.index_cids:
        logger.error(f"Index for {name} not found in global state")
        return None
    
    serialized_index = None
    index_cid = app.state.index_cids[name]

    # try fetch from IPFS using POST for the cat API
    try:
        ipfs_api_url = f"http://localhost:5001/api/v0/cat"
        logger.info(f"Requesting index from IPFS at URL: {ipfs_api_url} with CID: {index_cid}")
        # Fetch index from IPFS
        response = requests.post(ipfs_api_url, params={"arg": index_cid}, timeout=10)
        
        # Fetch chunk data from IPFS
        logger.info(response)
        if response.status_code == 200:
            serialized_index = response.content
        else:
            logger.error(f"Failed to retrieve index: {response.status_code} - {response.text}")
            serialized_index = None
    except Exception as e:
        logger.error(f"Error retrieving {name} index: {str(e)}")
        serialized_index = None

    if not serialized_index:
        logger.info(f"Index for {name} not found in IPFS")
        return None  # Return None instead of an uninitialized index
        
    index = CIDIndex()
    logger.info(f"Deserializing {name} index")
    try:
        index.load(io.BytesIO(serialized_index))
        logger.info(f"Index for {name} loaded successfully")
        return index
    except Exception as e:
        logger.error(f"Failed to deserialize index: {str(e)}")
        return None


def query_index(index, query, index_attribute) -> list:
    """
    Query the index for CIDs matching the given query, this function assumes the index_attribute condition is presented as an "and" condition in the query.
    Args:
        index (CIDIndex): The index to query
        query (str): The sql query string
        index_attribute (str): The attribute to use for querying
    Returns:
        list: List of CIDs matching the query for the default index attribute
    """

    # Parse the query to extract the attribute and value
    # Simplified version, only handles integer and string comparisons
    
    # Parse the WHERE clause from the query
    where_pattern = re.compile(r"where\s+(.*)", re.IGNORECASE)
    where_match = where_pattern.search(query)
    if not where_match:
        logger.info("No WHERE clause found in the query, retrieving all CIDs")
        cids = index.query_range()  # Query all CIDs
        return cids
    
    where_clause = where_match.group(1).strip()

    # Extract conditions related to the index_attribute
    conditions = []
    for condition in re.split(r"\s+and\s+", where_clause, flags=re.IGNORECASE):
        if index_attribute in condition:
            conditions.append(condition.strip())

    logger.info(f"Extracted conditions for index attribute '{index_attribute}': {conditions}")
    # print the index type
    logger.info(f"Index type: {index.index_type}")
    
    if not conditions:
        logger.error(f"No conditions found for index on '{index_attribute}', return all CIDs")
        cids = index.query_range()
        return cids

    # Process conditions and query the index
    cids = set()
    for condition in conditions:
        if ">=" in condition:
            key = condition.split(">=")[1].strip().strip("'\"")
            key = int(key) if index.index_type == "bplustree" else key
            cids.update(index.query_range(key))
        elif "<=" in condition:
            key = condition.split("<=")[1].strip().strip("'\"")
            key = int(key) if index.index_type == "bplustree" else key
            cids.update(index.query_range(-inf, key))
        elif ">" in condition:
            key = condition.split(">")[1].strip().strip("'\"")
            key = int(key) if index.index_type == "bplustree" else key
            cids.update(index.query_range(key+1, inf))
        elif "<" in condition:
            key = condition.split("<")[1].strip().strip("'\"")
            key = int(key) if index.index_type == "bplustree" else key
            cids.update(index.query_range(-inf, key-1))
        elif "=" in condition:
            key = condition.split("=")[1].strip().strip("'\"")
            key = int(key) if index.index_type == "bplustree" else key
            cids.update(index.query(key))
        elif "!=" in condition:
            key = condition.split("!=")[1].strip().strip("'\"")
            key = int(key) if index.index_type == "bplustree" else key
            all_cids = set(index.query_range(None, None))  # Query all CIDs
            excluded_cids = set(index.query(key))
            cids.update(all_cids - excluded_cids)
        else:
            raise ValueError(f"Unsupported condition format: {condition}")

    return list(cids)



# bplustree.py ......................
import bisect
import pickle
import io
'''
2024-02-24

Implement a simple function that builds an index over a given set of data, and query function.

Input: [(timestamp, CID), (timestamp, CID), [(timestamp, CID) â€¦ ]
CID: 256-bit string
flexible; reuse structure for other attributes
index over timestamp
'''

'''
For reusable structure, need to right a greater than and an equal to function
'''


class BPlusTree:
    class Node:
        def __init__(self, leaf):
            self.leaf = leaf
            self.keys = []
            self.children = [None] if leaf else []
        
        def info(self):
            a = "Node: " + str(id(self))
            b = "Leaf: " + str(self.leaf)
            c = f'keys: {self.keys}'
            d = f'data: {self.children}'
            return f'{a}\n{b}\n{c}\n{d}\n'

    def create_node(self, leaf, keys, data):
        node = self.Node(leaf)
        node.keys = keys
        node.children = data
        return node
    
    def __init__(self, data, BLOCK_SIZE: int, key_type: type, value_type: type):
        self.BLOCK_SIZE = BLOCK_SIZE
        self.root = BPlusTree.Node(leaf=True)
        self.key_type = key_type
        self.value_type = value_type
        for timestamp, cid in data:
            self.insert((timestamp, cid))

    def insert_inorder(self, node: Node, key, child):        
        if node.leaf:
            # In the case of leaf: find where key should be indexed, insert key and child there
            index = bisect.bisect_right(node.keys, key)
            if index > 0 and node.keys[index-1] == key:
                node.children[index-1].append(child)
            else:
                node.keys.insert(index, key)
                node.children.insert(index, [child])
        else:
            # In the case of internal: find where key should be indexed, insert key and add right child after; left child was edited as the node before
            index = bisect.bisect_right(node.keys, key)
            node.keys.insert(index, key)
            node.children.insert(index+1, child)


    def split_node(self, node: Node) -> tuple[Node, Node, int]:
        # turn node into left node, create new right node, split up info. if internal node lift up the middle node to be divider
        keys = node.keys
        children = node.children
        left = node
        left.keys = []
        left.children = []
        n = len(keys)

        if left.leaf:
            right = BPlusTree.Node(leaf=True)
            left.keys = keys[:(n+1)//2]
            right.keys = keys[(n+1)//2:]
            left.children = children[:(n+1)//2]
            left.children.append(right)
            right.children = children[(n+1)//2:]
            return (left, right, keys[(n+1)//2])
        else:
            right = BPlusTree.Node(leaf=False)
            left.keys = keys[:(n+1)//2]
            right.keys = keys[(n+1)//2+1:]
            left.children = children[:(n+1)//2+1]
            right.children = children[(n+1)//2+1:]
            return (left, right, keys[(n+1)//2])

    # TODO: def remove(self, data):
    def remove(self, data):
        
        stack = []
        node = self.root
        while not node.leaf:
            i = 0
            for key in node.keys:
                if data[0] < key:
                    break
                i+=1
            stack.append(node)
            node = node.children[i]
        key, value = data[0], data[1]
        i = 0
        while i < len(node.keys) and key > node.keys[i]:
            i += 1
        if i == len(node.keys):
            return False # no data with that key found
        if node.keys[i] != key:
            return False # no data with that key found
        if value not in node.children[i]:
            return False # no data with that value found
        if len(node.children[i]) == 1:
            node.children = node.children[:i] + node.children[i+1:]
            node.keys = node.keys[:i] + node.keys[i+1:]
        else:
            node.children[i] = list(filter(lambda a: a != value, node.children[i]))
        # TODO: need to remove node if node is empty
        return True

    def insert(self, data):
        if self.key_type == None and self.value_type == None:
            self.key_type, self.value_type = type(data[0]), type(data[1])
        if type(data[0])!= self.key_type:
            raise TypeError(f"Expected {self.key_type} for key and got {type(data[0])}")
        if type(data[1])!= self.value_type:
            raise TypeError(f"Expected {self.value_type} for value and got {type(data[1])}")
        stack = []
        node = self.root

        # search for where it's supposed to go
        while not node.leaf:
            i = 0
            for key in node.keys:
                if data[0] < key:
                    break
                i+=1
            stack.append(node)
            node = node.children[i]

        # insert data in order
        self.insert_inorder(node, data[0], data[1])

        # split node if necessary, all the way back up to the top
        interceptor = None
        if len(node.keys) > self.BLOCK_SIZE:
            left, right, interceptor = self.split_node(node)
        while interceptor != None:
            if not stack:
                node = BPlusTree.Node(False)
                node.keys.append(interceptor)
                node.children.append(left)
                node.children.append(right)
                interceptor = None
                self.root = node
            else:
                node = stack.pop()
                self.insert_inorder(node, interceptor, right)
                interceptor = None
                if len(node.keys) > self.BLOCK_SIZE:
                    left, right, interceptor = self.split_node(node)

    def query(self, query):
        return self.queryRange(query, query)

    def queryRange(self, querystart, queryend):
        node = self.root
        while not node.leaf:
            i = 0
            for key in node.keys:
                if querystart <= key:
                    break
                i+=1
            node = node.children[i]
        i = 0

        # handle edge case of empty tree
        if not node.keys:
            return []
        
        # iterate till past start
        while node and querystart > node.keys[i]:
            i+=1
            if i == len(node.keys):
                node = node.children[-1]
                i = 0
            if not node:
                break

        # res = [] 
        res = set()
        # iterate till past end
        while node and queryend >= node.keys[i]:
            # res += node.children[i]
            res.update(node.children[i])
            i+=1
            if i == len(node.keys):
                node = node.children[-1]
                i = 0
            if not node:
                break
        return res

    def __str__(self):
        return 'BLOCK_SIZE: ' + str(self.BLOCK_SIZE) + '\n' + BPlusTree.dfs(self.root)
    
    @staticmethod
    def dfs(node: Node, tabs=0) -> str:
        res = ''
        if node.leaf:
            for _ in range(tabs):
                res += '        '
            res += str(node) + '\n'
            for i in range(len(node.keys)):
                for _ in range(tabs+1):
                    res += '        '
                res += str(node.keys[i])
                res += ' '
                res += str(node.children[i])
                res += '\n'
            for _ in range(tabs):
                res += '        '
            res += 'next: '
            res += str(node.children[-1])
            res += '\n'
        else:
            for _ in range(tabs):
                res += '        '
            res+=str(node) +'\n'
            res += BPlusTree.dfs(node.children[0], tabs+1)
            for i in range(len(node.keys)):
                for _ in range(tabs):
                    res += '        '
                res += str(node.keys[i])
                res +='\n'
                res += BPlusTree.dfs(node.children[i+1], tabs+1)
        return res

# build index based on input, -> index 
def buildIndex(data=[], block_size=10) -> BPlusTree:
    if not data:
        return BPlusTree(data, block_size, None, None)
    key_type, value_type = type(data[0][0]), type(data[0][1])
    index = BPlusTree(data, block_size, key_type, value_type)
    return index

# update index -> index
def updateIndex(index: BPlusTree, data) -> BPlusTree:
    for datum in data:
        index.insert(datum)
    return index

# point query -> list of all that match that query
def query(index: BPlusTree, time) -> list:
    return index.query(time)

# range query -> list of all that match query range
def queryRange(index: BPlusTree, startTime, endTime) -> list:
    return index.queryRange(startTime,endTime)

def dumpIndex(index: BPlusTree):
    stream = io.BytesIO()
    pickle.dump(index, stream)
    stream.seek(0)
    return stream
        

def readIndex(stream: io.BytesIO) -> BPlusTree:
    return pickle.load(stream)

# cidindex.py ....................
from math import inf
import bplustree
import trie
import io
import pickle
import numpy as np

# A universal index interface for both bplustree and trie

class CIDIndex:
    def __init__(self, data=None, block_size=10):
        """
        Initialize the CIDIndex. Automatically determines the index type based on the data type.
        
        Args:
            data (list): A list of tuples (key, value) to build the index.
            block_size (int): Block size for BPlusTree (only used for integer keys).
        """
        self.index = None
        self.index_type = None
        if data:
            # key_type = type(data[0][0])
            # print(f"Key type: {key_type}", data[0:5])
            if isinstance(data[0][0], (int, np.integer)):
                self.index = bplustree.buildIndex(data, block_size)
                self.index_type = "bplustree"
            elif isinstance(data[0][0], str):
                self.index = trie.buildIndex(data)
                self.index_type = "trie"
            else:
                raise TypeError("Unsupported key type. Only int and str are supported.")

    def update(self, data):
        """
        Update the index with new data.
         
        Args:
            data (list): A list of tuples (key, value) to update the index.
        """
        if not self.index:
            raise ValueError("Index is not initialized. Provide data during initialization.")
        
        if self.index_type == "bplustree":
            self.index = bplustree.updateIndex(self.index, data)
        elif self.index_type == "trie":
            self.index = trie.updateIndex(self.index, data)

    def query(self, key):
        """
        Perform a point query on the index.
        
        Args:
            key: The key to query.
        
        Returns:
            list: A list of values matching the key.
        """
        if not self.index:
            raise ValueError("Index is not initialized.")
        
        if self.index_type == "bplustree":
            return bplustree.query(self.index, key)
        elif self.index_type == "trie":
            return trie.query(self.index, key)

    def query_range(self, start_key=-inf, end_key=inf):
        """
        Perform a range query on the index.
        
        Args:
            start_key: The start key of the range.
            end_key: The end key of the range.
        
        Returns:
            list: A list of values matching the range.
        """
        if not self.index:
            raise ValueError("Index is not initialized.")
        
        if self.index_type == "bplustree":
            return bplustree.queryRange(self.index, start_key, end_key)
        elif self.index_type == "trie":
            return trie.queryRange(self.index, start_key, end_key)

    def dump(self):
        """
        Serialize the index to a binary stream.
        
        Returns:
            io.BytesIO: A binary stream containing the serialized index.
        """
        if not self.index:
            raise ValueError("Index is not initialized.")
        
        if self.index_type == "bplustree":
            return bplustree.dumpIndex(self.index)
        elif self.index_type == "trie":
            return trie.dumpIndex(self.index)

    def load(self, stream):
        """
        Load the index from a binary stream.
        
        Args:
            stream (io.BytesIO): A binary stream containing the serialized index.
        """
        if not stream:
            raise ValueError("Stream is empty.")
        
        try:
            index = pickle.load(stream)
            if isinstance(index, bplustree.BPlusTree):
                self.index = index
                self.index_type = "bplustree"
            elif isinstance(index, trie.CharTrie):
                self.index = index
                self.index_type = "trie"
            else:
                raise TypeError("Unsupported index type in the stream.")
        except Exception as e:
            raise ValueError(f"Failed to load index: {str(e)}")


# trie.py ..................
from pygtrie import CharTrie
import pickle
import io

# build index based on input, -> index 
def buildIndex(data = []) -> CharTrie:
    index = CharTrie()
    if data:
        index = updateIndex(index, data)
    return index

# update index -> index
def updateIndex(index: CharTrie, data) -> CharTrie:
    for key, value in data:
        if key not in index:
            index[key] = [value]
        else:
            index[key].append(value)
    return index

# point query -> list of all that match that query
def query(index: CharTrie, key) -> list:
    if key not in index:
        return []
    return index[key]

# range query -> Not sure if this should be like a prefix thing or smth. dunno rlly
def queryRange(index: CharTrie, startKey=None, endKey=None) -> list:
    vals = set()
    for key in index.keys():
        vals.update(index[key])
    return list(vals)
    
    # pass

def dumpIndex(index: CharTrie):
    stream = io.BytesIO()
    pickle.dump(index, stream)
    stream.seek(0)
    return stream
        

def readIndex(stream: io.BytesIO) -> CharTrie:
    return pickle.load(stream)


# Dockerfile for fastapi ...................
# Use a minimal Python image
FROM python:3.9-slim

# Install dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    default-jdk \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Set Java environment variables dynamically based on actual installation
RUN echo "Setting up Java environment..." && \
    JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:/bin/java::") && \
    if [ -z "$JAVA_HOME" ]; then \
        JAVA_HOME=$(find /usr/lib/jvm -name "java-*-openjdk-*" -type d | head -n 1); \
    fi && \
    echo "JAVA_HOME detected as $JAVA_HOME" && \
    echo "export JAVA_HOME=$JAVA_HOME" > /etc/profile.d/java.sh && \
    echo "export PATH=\$PATH:\$JAVA_HOME/bin" >> /etc/profile.d/java.sh && \
    echo "JAVA_HOME=$JAVA_HOME" >> /etc/environment

# ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
# For local testing
# ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64
# ENV PATH="${JAVA_HOME}/bin:${PATH}"

# Verify Java installation and show debug info
RUN echo "Verifying Java installation..." && \
    java -version && \
    echo "Java home is: $JAVA_HOME" && \
    ls -la $JAVA_HOME 2>/dev/null || echo "Warning: Cannot list JAVA_HOME directory"

# Install Python dependencies
RUN pip install --no-cache-dir fastapi uvicorn[standard] pyspark==3.3.2 pandas pyarrow requests python-multipart pygtrie python-dotenv

# Copy the FastAPI app
COPY app.py /app.py
COPY bplustree.py /bplustree.py
COPY cidindex.py /cidindex.py
COPY trie.py /trie.py

# Set environment variable for PySpark to find Java
ENV PYSPARK_SUBMIT_ARGS="--driver-java-options=-Dlog4j.logLevel=info pyspark-shell"

# Add a healthcheck for Java
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD java -version || exit 1

# Expose FastAPI port
EXPOSE 8000

# Run FastAPI with Uvicorn
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]


# driver.yml ....................

services:
  # FastAPI Driver Server (Spark Driver)
  fastapi:
    build: ./fastapi
    container_name: fastapi-driver
    network_mode: host
    environment:
      - SPARK_MASTER=spark://${SPARK_MASTER_HOST}:7077
      - SPARK_DRIVER_HOST=${SPARK_DRIVER_HOST}
    # ports:
    #   - "8000:8000" # FastAPI server port
    #   - "10025:10025" # Spark Blockmanager Port
    #   - "10026:10026" # Spark Driver-Blockmanager Port
    #   - "10027:10027" # Spark Driver-Executor Port
    volumes:
      - fastapi-temp:/data/chunks
    # networks:
    #   - app-net

  ipfs:
    image: ipfs/kubo:latest
    networks:
      - app-net
    container_name: ipfs-test
    ports:
      - "4001:4001"
      - "5001:5001"
      - "8082:8080" # Or another available port
    volumes:
      - ./data/ipfs:/data/ipfs

networks:
  app-net:
    driver: bridge
volumes:
  fastapi-temp:
  ipfs_data:

# .env file ......................
SPARK_MASTER_HOST=129.74.152.201
SPARK_DRIVER_HOST=129.74.152.201
SPARK_PUBLIC_DNS=129.74.152.201
SPARK_LOCAL_IP=129.74.152.201
SPARK_MASTER=spark://129.74.152.201:7077

# spark-master.yml ......................
services:
  # Spark Master Node
  spark-master:
    image: bitnami/spark:3.3.2
    container_name: spark-master
    network_mode: host
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=${SPARK_MASTER_HOST}
      - SPARK_PUBLIC_DNS=${SPARK_PUBLIC_DNS}
      - SPARK_LOCAL_IP=${SPARK_LOCAL_IP}
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_MASTER_WEBUI_PORT=8080

# worker.yml .................
services:
  spark-worker:
    image: bitnami/spark:3.3.2
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://${SPARK_MASTER_HOST}:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
    command: >
      bash -c "pip install pandas requests pyarrow && 
               bin/spark-class org.apache.spark.deploy.worker.Worker spark://${SPARK_MASTER_HOST}:7077"
    network_mode: host

  ipfs:
    image: ipfs/kubo:latest
    networks:
      - app-net
    container_name: ipfs-test
    ports:
      - "4001:4001"
      - "5001:5001"
      - "8082:8080" # Or another available port
    volumes:
      - ./data/ipfs:/data/ipfs

networks:
  app-net:
    driver: bridge
volumes:
  fastapi-temp:
  ipfs_data:


# start.sh ..................

#!/bin/bash

# Build and start spark-master containers
echo "Building and starting Spark master containers..."
sudo docker compose -f spark-master.yml build
sudo docker compose -f spark-master.yml up -d

# Build and start driver containers
echo "Building and starting Spark driver containers..."
sudo docker compose -f driver.yml build
sudo docker compose -f driver.yml up -d

# Build and start worker containers
echo "Building and starting Spark worker containers..."
sudo docker compose -f worker.yml build
sudo docker compose -f worker.yml up -d

echo "All containers have been built and started successfully."

# stop.sh ...............

#!/bin/bash

# Stop and remove containers defined in spark-master.yml
echo "Stopping Spark master containers..."
sudo docker compose -f spark-master.yml down

# Stop and remove containers defined in worker.yml
echo "Stopping Spark worker containers..."
sudo docker compose -f worker.yml down

# Stop and remove containers defined in driver.yml
echo "Stopping Spark driver containers..."
sudo docker compose -f driver.yml down

echo "All containers have been stopped successfully."

This is my fastapi application for web3db. I have linux machine with sgx enabled and gramine installed. How can I run the program inside gramine sgx?